import urllib
import requests
from pprint import pprint
from pathlib import Path
import csv
import time

# TASK 1

fema_hazards_zone_url = 'https://hazards.fema.gov/arcgis/rest/services/public/NFHL/MapServer/28/query'

petroleum_file_path = Path('/Users/sanyuktasingh/Desktop/data class/Petroleum Refineries by City.csv')

petroleum_table = []

with open(petroleum_file_path, 'r', encoding='cp1252') as petroleum_file_path:
    reader = csv.reader(petroleum_file_path)
    headers = next(reader)
    petroleum_table.append(headers + ['FEMA_Hazard_Zone']) #new column to put the flood risk data

    for row in reader: #go through each row in csv 
        latitude = row[headers.index('Latitude')] #setting variables equal to info in lat row 
        longitude = row[headers.index('Longitude')]
        
        query = {
            'geometry': f'{{"x": {longitude}, "y": {latitude}}}',
                'inSR':'4326',
                'geometryType':'esriGeometryPoint',
                'spatialRel': 'esriSpatialRelIntersects',
                'outFields': 'ZONE_SUBTY',
                'returnGeometry': 'false',
                'f': 'pjson'
                }

        encoded_query = urllib.parse.urlencode(query)
        full_url = fema_hazards_zone_url + "?" + encoded_query
        fema_hazards_zone_data = requests.get(url=full_url).json()
        time.sleep(0.5) # half-second pause after each request to prevent ConnectionResetError

        if len(fema_hazards_zone_data['features']) > 0:
            zone = fema_hazards_zone_data['features'][0]['attributes']['ZONE_SUBTY']

            if zone is None:
                    zone = 'No Data'
        else: zone = 'No Data'

        new_row = row + [zone]
        petroleum_table.append(new_row)

output_file = Path('/Users/sanyuktasingh/Desktop/data class/Task1_Week7.csv')
with open(output_file, 'w', newline="", encoding='cp1252') as f:
    writer = csv.writer(f)
    writer.writerows(petroleum_table)

#print(output_file)


# TASK 4 

osrm_url = 'https://routing.openstreetmap.de/routed-car/route/v1/driving/'

petroleum_table[0].append('DriveDuration_Seconds') #add that new column name to the end of that header list
    
updated_rows = [petroleum_table[0]]  # new list to hold the updated rows, not append to the same list we're looping through

for row in petroleum_table[1:]: #loop through all rows except the header 
    latitude = row[headers.index('Latitude')]
    longitude = row[headers.index('Longitude')]
    city_lat = row[headers.index('NearestMajorCity_Latitude')]
    city_lon = row[headers.index('NearestMajorCity_Longitude')]

    osrm_request_url = f'{osrm_url}{longitude},{latitude};{city_lon},{city_lat}?steps=false'
    osrm_response = requests.get(url=osrm_request_url).json()
    time.sleep(0.5) # half-second pause after each request to prevent ConnectionResetError

    if osrm_response.get('routes') and len(osrm_response['routes']) > 0:
        drive_duration = osrm_response['routes'][0]['duration'] #access value associated with key routes and take first element (so take first route) and inside that find duration

        if drive_duration is None:
             drive_duration = 'No Data'
    else:
         drive_duration = 'No Data'
    
    new_row = row + [drive_duration]
    updated_rows.append(new_row)

output_file = Path('/Users/sanyuktasingh/Desktop/data class/Task4_Week7.csv')
with open(output_file, 'w', newline="", encoding='cp1252') as f:
    writer = csv.writer(f)
    writer.writerows(updated_rows)