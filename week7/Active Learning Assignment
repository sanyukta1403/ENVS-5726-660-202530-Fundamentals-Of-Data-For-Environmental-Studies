import urllib
import requests
from pprint import pprint
from pathlib import Path
import csv
import time

# TASK 1

fema_hazards_zone_url = 'https://hazards.fema.gov/arcgis/rest/services/public/NFHL/MapServer/28/query'

petroleum_file_path = Path('/Users/sanyuktasingh/Desktop/data class/Petroleum Refineries by City.csv')

petroleum_table = []

with open(petroleum_file_path, 'r', encoding='cp1252') as petroleum_file_path:
    reader = csv.reader(petroleum_file_path)
    headers = next(reader)
    petroleum_table.append(headers + ['FEMA_Hazard_Zone']) #new column to put the flood risk data

    for row in reader: #go through each row in csv 
        latitude = row[headers.index('Latitude')] # gets the value in the lat column for the current row
        longitude = row[headers.index('Longitude')]
        
        query = { # creates a dictionary of query parameters that will be passed to the FEMA ArcGIS endpoint
            'geometry': f'{{"x": {longitude}, "y": {latitude}}}',
                'inSR':'4326',
                'geometryType':'esriGeometryPoint',
                'spatialRel': 'esriSpatialRelIntersects',
                'outFields': 'ZONE_SUBTY',
                'returnGeometry': 'false',
                'f': 'pjson'
                }

        encoded_query = urllib.parse.urlencode(query) # encodes the query parameters into a URL-friendly string
        full_url = fema_hazards_zone_url + "?" + encoded_query # combines the base URL and encoded query string into a full request URL
        fema_hazards_zone_data = requests.get(url=full_url).json()  # sends a GET request to the FEMA API and parses the JSON response

        if len(fema_hazards_zone_data['features']) > 0: # checks if the API response includes any results in the “features” list
            zone = fema_hazards_zone_data['features'][0]['attributes']['ZONE_SUBTY']  # xxtract the flood zone subtype from the first feature

            if zone is None:
                    zone = 'No Data'
        else: zone = 'No Data'

        new_row = row + [zone]
        petroleum_table.append(new_row)

output_file = Path('/Users/sanyuktasingh/Desktop/data class/Task1_Week7.csv')
with open(output_file, 'w', newline="", encoding='cp1252') as f:
    writer = csv.writer(f)
    writer.writerows(petroleum_table)

print(output_file)


# TASK 4 

osrm_url = 'https://routing.openstreetmap.de/routed-car/route/v1/driving/'

petroleum_table[0].append('DriveDuration_Seconds') #add that new column name to the end of that header list
    
updated_rows = [petroleum_table[0]]  # new list to hold the updated rows, not append to the same list we're looping through

for row in petroleum_table[1:]: #loop through all rows except the header 
    latitude = row[headers.index('Latitude')] #get refinery's lat 
    longitude = row[headers.index('Longitude')]
    city_lat = row[headers.index('NearestMajorCity_Latitude')] #get lat of nearest city 
    city_lon = row[headers.index('NearestMajorCity_Longitude')]

    osrm_request_url = f'{osrm_url}{longitude},{latitude};{city_lon},{city_lat}?steps=false'
    osrm_response = requests.get(url=osrm_request_url).json()
    time.sleep(0.5) # half-second pause after each request to prevent ConnectionResetError

    if osrm_response.get('routes') and len(osrm_response['routes']) > 0:   # checks if the response contains route data
        drive_duration = osrm_response['routes'][0]['duration'] #access value associated with key routes and take first element (so take first route) and inside that find duration

        if drive_duration is None: #if duration value missing give No Data
             drive_duration = 'No Data'
    else:
         drive_duration = 'No Data' #if no route found give No Data
    
    new_row = row + [drive_duration]
    updated_rows.append(new_row)

output_file = Path('/Users/sanyuktasingh/Desktop/data class/Task4_Week7.csv')
with open(output_file, 'w', newline="", encoding='cp1252') as f:
    writer = csv.writer(f)
    writer.writerows(updated_rows)