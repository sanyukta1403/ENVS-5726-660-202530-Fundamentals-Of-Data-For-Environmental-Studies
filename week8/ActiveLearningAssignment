from pathlib import Path
import csv
from collections import namedtuple
from pprint import pprint
import pandas as pd
from typing import List
import sys

# TASK 1

wash_file_path = '/Users/sanyuktasingh/Desktop/data class/week8/Rohingya WASH Survey.csv'

with open(wash_file_path, 'r', encoding='cp1252') as wash_file:
    reader = csv.reader(wash_file)
    headers = next(reader)

    WashSurvey = namedtuple(typename='WashSurvey', field_names = headers)

    Wash_Survey_table: List[WashSurvey] = []

    for row in reader:
        wash_survey = WashSurvey(*row)
        Wash_Survey_table.append(wash_survey)

#for row in Wash_Survey_table[:2]:
    #print(row)

import_to_panda = pd.DataFrame.from_records(
    data = Wash_Survey_table,
    columns= WashSurvey._fields
)

#print(import_to_panda.head())

filtered_for_g_df = import_to_panda.filter(regex='^G')

filtered_g_table = list(filtered_for_g_df.itertuples(name='WashSurvey', index=False))
#print(filtered_g_table[0])

# TASK 2

sys.path.append('/Users/sanyuktasingh/Desktop/data class/week8')

from datalib import convert_yesno_to_binary

g_and_binary_table = convert_yesno_to_binary(filtered_g_table)

#print(g_and_binary_table[0]) 

# TASK 3

from datalib import convert_str_to_numeric

new_table = convert_str_to_numeric(g_and_binary_table)
#print(new_table[0])

# TASK 4 

# even though the table does not have NaN values, the FactorAnalyzer function creates NaNs or infinite values internally when it tries to compute a correlation matrix using columns that have zero variance or are perfectly correlated 
# the solution is to standardize the data before performing factor analysis so that each variable contributes equally to the analysis
# https://stackoverflow.com/questions/56722430/factor-analysis-using-python-factor-analyzer 

# TASK 5 

import numpy as np
import pandas as pd

# Convert clean list of NamedTuples into a Pandas DataFrame
df_numeric = pd.DataFrame.from_records(new_table)
df_numeric = df_numeric.apply(pd.to_numeric, errors='coerce')

# Create correlation matrix (this is the table with NaN values)
corr_matrix = df_numeric.corr()
print("\n--- Correlation matrix with NaN values ---")
print(corr_matrix)

# Remove rows/columns that are entirely NaN
corr_clean = corr_matrix.dropna(axis=0, how='all').dropna(axis=1, how='all')


# ---------------------------------------------------------------------------
# Define the findCorrelation() function (matches the second script exactly)
# ---------------------------------------------------------------------------
def findCorrelation(corr, cutoff=0.9, exact=None):
    """
    This function is the Python implementation of the R function 
    `findCorrelation()`.

    Relies on numpy and pandas, so must have them pre-installed.

    It searches through a correlation matrix and returns a list of column names 
    to remove to reduce pairwise correlations.

    For the documentation of the R function, see 
    https://www.rdocumentation.org/packages/caret/topics/findCorrelation
    and for the source code of `findCorrelation()`, see
    https://github.com/topepo/caret/blob/master/pkg/caret/R/findCorrelation.R

    -----------------------------------------------------------------------------
    Parameters:
    -----------
    corr: pandas dataframe.
        A correlation matrix as a pandas dataframe.
    cutoff: float, default: 0.9.
        A numeric value for the pairwise absolute correlation cutoff
    exact: bool, default: None
        A boolean value that determines whether the average correlations be 
        recomputed at each step
    -----------------------------------------------------------------------------
    Returns:
    --------
    list of column names
    -----------------------------------------------------------------------------
    Example:
    --------
    R1 = pd.DataFrame({
        'x1': [1.0, 0.86, 0.56, 0.32, 0.85],
        'x2': [0.86, 1.0, 0.01, 0.74, 0.32],
        'x3': [0.56, 0.01, 1.0, 0.65, 0.91],
        'x4': [0.32, 0.74, 0.65, 1.0, 0.36],
        'x5': [0.85, 0.32, 0.91, 0.36, 1.0]
    }, index=['x1', 'x2', 'x3', 'x4', 'x5'])

    findCorrelation(R1, cutoff=0.6, exact=False)  # ['x4', 'x5', 'x1', 'x3']
    findCorrelation(R1, cutoff=0.6, exact=True)   # ['x1', 'x5', 'x4'] 
    """

    def _findCorrelation_fast(corr, avg, cutoff):
        combsAboveCutoff = corr.where(lambda x: (np.tril(x) == 0) & (x > cutoff)).stack().index
        rowsToCheck = combsAboveCutoff.get_level_values(0)
        colsToCheck = combsAboveCutoff.get_level_values(1)
        msk = avg[colsToCheck] > avg[rowsToCheck].values
        deletecol = pd.unique(np.r_[colsToCheck[msk], rowsToCheck[~msk]]).tolist()
        return deletecol

    def _findCorrelation_exact(corr, avg, cutoff):
        x = corr.loc[(*[avg.sort_values(ascending=False).index]*2,)]

        # Add type safety for integer-only DataFrames
        if (x.dtypes.values[:, None] == ['int64', 'int32', 'int16', 'int8']).any():
            x = x.astype(float)

        x.values[(*[np.arange(len(x))]*2,)] = np.nan

        deletecol = []
        for ix, i in enumerate(x.columns[:-1]):
            for j in x.columns[ix+1:]:
                if x.loc[i, j] > cutoff:
                    if x[i].mean() > x[j].mean():
                        deletecol.append(i)
                        x.loc[i] = x[i] = np.nan
                    else:
                        deletecol.append(j)
                        x.loc[j] = x[j] = np.nan
        return deletecol

    # Ensure correlation matrix is symmetric
    if not np.allclose(corr, corr.T) or any(corr.columns != corr.index):
        raise ValueError("correlation matrix is not symmetric.")

    acorr = corr.abs()
    avg = acorr.mean()

    # Choose method based on matrix size and exact flag
    if exact or exact is None and corr.shape[1] < 100:
        return _findCorrelation_exact(acorr, avg, cutoff)
    else:
        return _findCorrelation_fast(acorr, avg, cutoff)


# ---------------------------------------------------------------------------
# Apply correlation filter and clean the DataFrame
# ---------------------------------------------------------------------------
high_corr_cols = findCorrelation(corr_clean, cutoff=0.9)
print("\nHighly correlated columns to remove:", high_corr_cols)

df_final = df_numeric.drop(columns=high_corr_cols, errors='ignore').dropna()
print("\n--- Final cleaned DataFrame ---")
print(df_final.head())


# The code follows the same steps as the Stack_Overflow_Factor_Analysis_Debug_2.html example, which demonstrates the use of a correlation matrix and the findCorrelation() function to identify and remove highly correlated variables before factor analysis. I replicated those steps in Python by creating the correlation matrix from my cleaned DataFrame, observing NaN values for non-variable or missing data, cleaning the matrix, and applying the same findCorrelation() logic shown in the HTML.
 
# TASK 6 

from factor_analyzer import FactorAnalyzer

# remove columns that are constant (zero variance)
df_final = df_final.loc[:, df_final.std() > 0]

# remove any rows with NaN or infinite values
df_final = df_final.replace([np.inf, -np.inf], np.nan).dropna()

print("\nAfter final cleaning for Factor Analysis:")
print(df_final.shape)
print(df_final.head())

fa = FactorAnalyzer(rotation="varimax")
fa.fit(df_final)

number_of_factors = 5
fa = FactorAnalyzer(n_factors=number_of_factors, rotation="varimax")
fa.fit(df_final)

factor_names = [f'Factor{i+1}' for i in range(number_of_factors)]

loadings_df = pd.DataFrame(fa.loadings_, columns=factor_names, index=[df_final.columns])

index_names = ['Sum of Squared Loadings', 'Proportional Variance', 'Cumulative Variance']
variance_df = pd.DataFrame(fa.get_factor_variance(), columns=factor_names, index=index_names)

loadings_df.to_csv('/Users/sanyuktasingh/Desktop/data class/week8/WASH_Loadings')
variance_df.to_csv('/Users/sanyuktasingh/Desktop/data class/week8/WASH_Variance')