from pathlib import Path
import csv
from collections import namedtuple
from pprint import pprint
import pandas as pd
from typing import List
import sys

# TASK 1

wash_file_path = '/Users/sanyuktasingh/Desktop/data class/week8/Rohingya WASH Survey.csv'

with open(wash_file_path, 'r', encoding='cp1252') as wash_file:
    reader = csv.reader(wash_file)
    headers = next(reader)

    WashSurvey = namedtuple(typename='WashSurvey', field_names = headers) # creates a named tuple class called WashSurvey using the header names as field names; access data by attribute name

    Wash_Survey_table: List[WashSurvey] = [] #initializes an empty list that will store all the rows as WashSurvey objects

    for row in reader: 
        wash_survey = WashSurvey(*row) #creates a new WashSurvey namedtuple by unpacking all columns in that row into the corresponding fields
        Wash_Survey_table.append(wash_survey)

#for row in Wash_Survey_table[:2]:
    #print(row)

import_to_panda = pd.DataFrame.from_records( #converts the list of namedtuples into a Pandas DataFrame
    data = Wash_Survey_table,
    columns= WashSurvey._fields #gives the DataFrame the same column names as the CSV headers (namedtuple fields)
)

#print(import_to_panda.head()) #prints the first 5 rows of your DataFrame 

filtered_for_g_df = import_to_panda.filter(regex='^G')

filtered_g_table = list(filtered_for_g_df.itertuples(name='WashSurvey', index=False))
print(filtered_g_table[0])

# TASK 2

sys.path.append('/Users/sanyuktasingh/Desktop/data class/week8') #adds a new directory path to Pythonâ€™s module search path

from datalib import convert_yesno_to_binary #imports the specific function from datalib

g_and_binary_table = convert_yesno_to_binary(filtered_g_table)

#print(g_and_binary_table[0]) 

# TASK 3

from datalib import convert_str_to_numeric

new_table = convert_str_to_numeric(g_and_binary_table)
print(new_table[0])

"""""
# TASK 4 

even though the table does not have NaN values, the FactorAnalyzer function creates NaNs or infinite values internally when it tries to compute a correlation matrix using columns that have zero variance or are perfectly correlated 
the solution is to standardize the data before performing factor analysis so that each variable contributes equally to the analysis
https://stackoverflow.com/questions/56722430/factor-analysis-using-python-factor-analyzer 

"""
# TASK 5 

import numpy as np
import pandas as pd

# Convert clean list of NamedTuples into a Pandas DataFrame
df_numeric = pd.DataFrame.from_records(new_table)
df_numeric = df_numeric.apply(pd.to_numeric, errors='coerce')

# Create correlation matrix (this is the table with NaN values)
corr_matrix = df_numeric.corr()
print("\n--- Correlation matrix with NaN values ---")
print(corr_matrix)

corr_clean = corr_matrix.dropna(axis=0, how='all').dropna(axis=1, how='all') # Remove rows/columns that are entirely NaN


def findCorrelation(corr, cutoff=0.9, exact=None): # identifies and removes columns that are too highly correlated with each other

    def _findCorrelation_fast(corr, avg, cutoff): # defines a helper function to quickly find highly correlated columns to remove
        combsAboveCutoff = corr.where(lambda x: (np.tril(x) == 0) & (x > cutoff)).stack().index # finds all unique pairs of columns whose correlation is above the cutoff 
        rowsToCheck = combsAboveCutoff.get_level_values(0) # extracts the first column names from those correlated pairs
        colsToCheck = combsAboveCutoff.get_level_values(1) # extracts the second column names from those correlated pairs
        msk = avg[colsToCheck] > avg[rowsToCheck].values # compares average correlations to decide which column in each pair is more correlated overall
        deletecol = pd.unique(np.r_[colsToCheck[msk], rowsToCheck[~msk]]).tolist() # collects the columns to drop and removes dupplicates
        return deletecol # returns the list of column names to remove

    def _findCorrelation_exact(corr, avg, cutoff): # defines a slower but more precise method for finding highly correlated columns
        x = corr.loc[(*[avg.sort_values(ascending=False).index]*2,)] # reorders the correlation matrix so columns with higher average correlations come first

        # converts integer data to float type for safety in calculations
        if (x.dtypes.values[:, None] == ['int64', 'int32', 'int16', 'int8']).any():
            x = x.astype(float)

        x.values[(*[np.arange(len(x))]*2,)] = np.nan # sets the self-correlations to NaN to ignore them

        deletecol = [] # Creates an empty list to store names of columns to remove
        for ix, i in enumerate(x.columns[:-1]): # Loops through each column except the last
            for j in x.columns[ix+1:]: # compares each column with every column that comes after it
                if x.loc[i, j] > cutoff: # checks if two columns have a correlation above the cutoff
                    if x[i].mean() > x[j].mean(): # compares which column is more correlated on average
                        deletecol.append(i)
                        x.loc[i] = x[i] = np.nan # marks column i for deletion and ignores it for future comparisons
                    else: # else marks column j for deletion and ignores it
                        deletecol.append(j)
                        x.loc[j] = x[j] = np.nan
        return deletecol # returns the list of column names to remove

    # checks if correlation matrix is symmetric; otherwise raise error 
    if not np.allclose(corr, corr.T) or any(corr.columns != corr.index):
        raise ValueError("correlation matrix is not symmetric.")

    acorr = corr.abs() # converts all correlation values to positive (absolute) form
    avg = acorr.mean() # calculates the average correlation for each column

    # Choose method based on matrix size and exact flag
    if exact or exact is None and corr.shape[1] < 100:
        return _findCorrelation_exact(acorr, avg, cutoff)
    else:
        return _findCorrelation_fast(acorr, avg, cutoff)


high_corr_cols = findCorrelation(corr_clean, cutoff=0.9)
print("\nHighly correlated columns to remove:", high_corr_cols)

df_final = df_numeric.drop(columns=high_corr_cols, errors='ignore').dropna()
print("\n--- Final cleaned DataFrame ---")
print(df_final.head())


# The code follows the same steps as the Stack_Overflow_Factor_Analysis_Debug_2.html example, which demonstrates the use of a correlation matrix and the findCorrelation() function to identify and remove highly correlated variables before factor analysis. I replicated those steps in Python by creating the correlation matrix from my cleaned DataFrame, observing NaN values for non-variable or missing data, cleaning the matrix, and applying the same findCorrelation() logic shown in the HTML.

#The table with NaN values is the correlation matrix, which shows how strongly each variable is related to every other variable in the survey dataset. Some cells contain NaN because certain columns either had no variation (e.g., all responses were identical) or contained non-numeric/missing data that could not be used in correlation calculations. By cleaning the data and removing these problematic columns or rows, we obtained a complete numeric dataset suitable for further analysis.
 
# TASK 6 

from factor_analyzer import FactorAnalyzer

df_final = df_final.loc[:, df_final.std() > 0] # remove columns that are constant (zero variance)


df_final = df_final.replace([np.inf, -np.inf], np.nan).dropna() # remove any rows with NaN or infinite values

print("\nAfter final cleaning for Factor Analysis:")
print(df_final.shape)
print(df_final.head())

fa = FactorAnalyzer(rotation="varimax")
fa.fit(df_final)

number_of_factors = 5
fa = FactorAnalyzer(n_factors=number_of_factors, rotation="varimax")
fa.fit(df_final)

factor_names = [f'Factor{i+1}' for i in range(number_of_factors)]

loadings_df = pd.DataFrame(fa.loadings_, columns=factor_names, index=[df_final.columns])

index_names = ['Sum of Squared Loadings', 'Proportional Variance', 'Cumulative Variance']
variance_df = pd.DataFrame(fa.get_factor_variance(), columns=factor_names, index=index_names)

loadings_df.to_csv('/Users/sanyuktasingh/Desktop/data class/week8/WASH_Loadings')
variance_df.to_csv('/Users/sanyuktasingh/Desktop/data class/week8/WASH_Variance')

# the factor analysis groups together responses (instead of individual Qs) that explain the patterns in the data; access and infrastructure conditions explains the largest share of variation 